{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from stereoVO.structures import StateBolts, VO_StateMachine\n",
    "from stereoVO.datasets import KittiDataset\n",
    "from stereoVO.configs import yaml_parser\n",
    "from stereoVO.optimization import get_minimization\n",
    "from stereoVO.geometry import (DetectionEngine, \n",
    "                               filter_matching_inliers, \n",
    "                               triangulate_points, \n",
    "                               filter_triangulated_points,\n",
    "                               project_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../KITTI/KITTI_gray/dataset/sequences/00/'\n",
    "CONFIG_PATH =  '../configs/params.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KittiDataset(PATH)\n",
    "\n",
    "dataset.intrinsic = dataset.camera_intrinsic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml_parser(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise the State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_num = 0\n",
    "\n",
    "left_frame, right_frame, ground_truth = dataset[state_num]\n",
    "\n",
    "#initialise the state\n",
    "prevState = VO_StateMachine(state_num)\n",
    "\n",
    "#set frame state\n",
    "prevState.frames = left_frame, right_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detction Engine, Matching and Triangulation for first frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_engine = DetectionEngine(prevState.frames.left, prevState.frames.right, params)\n",
    "\n",
    "prevState.matchedPoints, prevState.keypoints, prevState.descriptors = detection_engine.get_matching_keypoints()\n",
    "\n",
    "prevState.inliers, mask_epipolar = filter_matching_inliers(prevState.matchedPoints.left,\n",
    "                                            prevState.matchedPoints.right,\n",
    "                                            dataset.intrinsic,\n",
    "                                            params)\n",
    "\n",
    "prevState.pts3D, reproj_error = triangulate_points(prevState.inliers.left,\n",
    "                                    prevState.inliers.right,\n",
    "                                    dataset.PL,\n",
    "                                    dataset.PR)\n",
    "\n",
    "args_triangulation = params.geometry.triangulation\n",
    "prevState.pts3D_Filter, maskTriangulationFilter, ratioFilter = filter_triangulated_points(prevState.pts3D, reproj_error, **args_triangulation)\n",
    "\n",
    "prevState.InliersFilter = prevState.inliers.left[maskTriangulationFilter], prevState.inliers.right[maskTriangulationFilter]\n",
    "prevState.ratioTriangulationFilter = ratioFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise the Second State and set it as currState**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_num = 1\n",
    "\n",
    "left_frame, right_frame, ground_truth = dataset[state_num]\n",
    "\n",
    "#initialise the state\n",
    "currState = VO_StateMachine(state_num)\n",
    "\n",
    "#set frame state\n",
    "currState.frames = left_frame, right_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_engine = DetectionEngine(currState.frames.left, currState.frames.right, params)\n",
    "\n",
    "currState.matchedPoints, currState.keypoints, currState.descriptors = detection_engine.get_matching_keypoints()\n",
    "\n",
    "currState.inliers, mask_epipolar = filter_matching_inliers(currState.matchedPoints.left,\n",
    "                                            currState.matchedPoints.right,\n",
    "                                            dataset.intrinsic,\n",
    "                                            params)\n",
    "\n",
    "currState.pts3D, reproj_error = triangulate_points(currState.inliers.left,\n",
    "                                    currState.inliers.right,\n",
    "                                    dataset.PL,\n",
    "                                    dataset.PR)\n",
    "\n",
    "args_triangulation = params.geometry.triangulation\n",
    "currState.pts3D_Filter, maskTriangulationFilter, ratioFilter = filter_triangulated_points(currState.pts3D, reproj_error, **args_triangulation)\n",
    "\n",
    "currState.InliersFilter = currState.inliers.left[maskTriangulationFilter], currState.inliers.right[maskTriangulationFilter]\n",
    "currState.ratioTriangulationFilter = ratioFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracker = TrackingEngine(prev_frames, curr_frames, prevState.InliersFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevFrames = prevState.frames\n",
    "currFrames = currState.frames\n",
    "prevInliers = prevState.InliersFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Tracking from prev state to current state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_params = dict(winSize  = (21, 21), \n",
    "                 maxLevel = 5,\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.03))\n",
    "\n",
    "dist_x_y = lambda x,y:np.sqrt(np.sum((x - y)**2, axis=1))\n",
    "\n",
    "def feature_tracking(imageRef, imageCur, pointsRef):\n",
    "    \n",
    "    \"\"\"\n",
    "    To Do : Add docstring for the function here.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(pointsRef.shape)==2 and pointsRef.shape[1]==2\n",
    "    \n",
    "    pointsRef = pointsRef.reshape(-1,1,2).astype('float32')\n",
    "    points_t0_t1, mask_t0_t1, err = cv2.calcOpticalFlowPyrLK(imageRef, imageCur, pointsRef, None, **lk_params)\n",
    "    \n",
    "    pointsRef = pointsRef.reshape(-1,2)\n",
    "    points_t0_t1 = points_t0_t1.reshape(-1,2)\n",
    "    mask_t0_t1 = mask_t0_t1.flatten().astype(bool)\n",
    "       \n",
    "#     pointsRef = pointsRef[mask_t0_t1]\n",
    "#     points_t0_t1 = points_t0_t1[mask_t0_t1]\n",
    "    \n",
    "    return pointsRef, points_t0_t1, mask_t0_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsFilterLeft, pointsTrackedLeft, maskTrackingLeft = feature_tracking(prevFrames.left,\n",
    "                                                                        currFrames.left,\n",
    "                                                                        prevInliers.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsFilterRight, pointsTrackedRight, maskTrackingRight = feature_tracking(prevFrames.right,\n",
    "                                                                           currFrames.right,\n",
    "                                                                           prevInliers.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint index and select only good tracked points\n",
    "maskTracking = np.logical_and(maskTrackingLeft, maskTrackingRight)\n",
    "pointsTracked = StateBolts(pointsTrackedLeft[maskTracking], pointsTrackedRight[maskTracking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-valid points from inliers filtered in prev state\n",
    "prevStateInliers = StateBolts(prevInliers.left[maskTracking], prevInliers.right[maskTracking])\n",
    "pts3D_TrackingFilter = prevState.pts3D_Filter[maskTracking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, __ ) , mask_tracking_epipolar_left = filter_matching_inliers(prevStateInliers.left, pointsTracked.left, dataset.intrinsic, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, __ ) , mask_tracking_epipolar_right = filter_matching_inliers(prevStateInliers.right, pointsTracked.right, dataset.intrinsic, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tracking_epipolar = np.logical_and(mask_tracking_epipolar_left, mask_tracking_epipolar_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currState.pointsTracked = (pointsTracked.left[mask_tracking_epipolar], pointsTracked.right[mask_tracking_epipolar])\n",
    "prevState.inliersTrackingFilter =  (prevStateInliers.left[mask_tracking_epipolar], prevStateInliers.right[mask_tracking_epipolar])\n",
    "prevState.pts3D_TrackingFilter = pts3D_TrackingFilter[mask_tracking_epipolar]                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P3P Solver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale of translation of camera     : 0.6764176064655109\n",
      "Solution obtained in P3P Iteration : 1\n",
      "Ratio of Inliers                   : 1.0\n"
     ]
    }
   ],
   "source": [
    "args_pnpSolver = params.geometry.pnpSolver\n",
    "\n",
    "\n",
    "for i in range(args_pnpSolver.numTrials):\n",
    "    \n",
    "    retval, r_vec1, t_vec1, idxPose = cv2.solvePnPRansac(prevState.pts3D_TrackingFilter,\n",
    "                                                       currState.pointsTracked.left,\n",
    "                                                       dataset.intrinsic,\n",
    "                                                       None,\n",
    "                                                       iterationsCount=args_pnpSolver.numTrials,\n",
    "                                                       reprojectionError=args_pnpSolver.reprojectionError,\n",
    "                                                       confidence=args_pnpSolver.confidence,\n",
    "                                                       flags=cv2.SOLVEPNP_P3P)\n",
    "    \n",
    "    \n",
    "    r_mat, _ = cv2.Rodrigues(r_vec1)\n",
    "    \n",
    "    # r_vec and t_vec obtained are in camera coordinate frames\n",
    "    # we need to convert these matrix in world coordinates system\n",
    "    # or we need to transforms the matrix from currState to prevState\n",
    "    r_mat = r_mat.T\n",
    "    t_vec = -r_mat.T @ t_vec1\n",
    "    \n",
    "    idxPose = idxPose.flatten().astype(bool)\n",
    "    \n",
    "    ratio = sum(idxPose)/len(idxPose)\n",
    "    scale = np.linalg.norm(t_vec)\n",
    "    \n",
    "    if scale<args_pnpSolver.deltaT and ratio>args_pnpSolver.minRatio:\n",
    "        print(\"Scale of translation of camera     : {}\".format(scale))\n",
    "        print(\"Solution obtained in P3P Iteration : {}\".format(i+1))\n",
    "        print(\"Ratio of Inliers                   : {}\".format(ratio))\n",
    "        break\n",
    "    else:\n",
    "        print(\"Warning : Max Iter : {} reached, still large position delta produced\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear Least Square Optimization of Pose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'project_points' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-dae89eea0b06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Prepare the solver for minimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     optRes = least_squares(get_minimization, doF, method='lm', max_nfev=2000,\n\u001b[0m\u001b[1;32m     15\u001b[0m                             args=(prevState.pts3D_TrackingFilter, \n\u001b[1;32m     16\u001b[0m                                   \u001b[0mcurrState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpointsTracked\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opencv/lib/python3.8/site-packages/scipy/optimize/_lsq/least_squares.py\u001b[0m in \u001b[0;36mleast_squares\u001b[0;34m(fun, x0, jac, bounds, method, ftol, xtol, gtol, x_scale, loss, f_scale, diff_step, tr_solver, tr_options, jac_sparsity, max_nfev, verbose, args, kwargs)\u001b[0m\n\u001b[1;32m    815\u001b[0m         \u001b[0mx0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_strictly_feasible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 817\u001b[0;31m     \u001b[0mf0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/opencv/lib/python3.8/site-packages/scipy/optimize/_lsq/least_squares.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'trf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SFM_VO_Pytorch/stereoVO/optimization/minimization.py\u001b[0m in \u001b[0;36mget_minimization\u001b[0;34m(dof, points3D_world, keypointsLeft, keypointsRight, PL, PR)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpts3D_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT_mat\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mpoints3D_world\u001b[0m \u001b[0;31m#shape:(3xN)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mpts2D_projection_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts3D_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m     \u001b[0mpts2D_projection_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproject_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpts3D_left\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'project_points' is not defined"
     ]
    }
   ],
   "source": [
    "optimisation=True\n",
    "\n",
    "if optimisation:\n",
    "    \n",
    "    # Convert the matrix from world coordinates(prevState) to camera coordinates (currState)\n",
    "    r_mat = r_mat.T\n",
    "    t_vec = -r_mat.T@t_vec\n",
    "    r_vec, _ = cv2.Rodrigues(r_mat)\n",
    "    \n",
    "    # Prepare an initial set of parameters to the optimizer\n",
    "    doF = np.concatenate((r_vec, t_vec)).flatten()\n",
    " \n",
    "    # Prepare the solver for minimization\n",
    "    optRes = least_squares(get_minimization, doF, method='lm', max_nfev=2000,\n",
    "                            args=(prevState.pts3D_TrackingFilter, \n",
    "                                  currState.pointsTracked.left, \n",
    "                                  currState.pointsTracked.right, \n",
    "                                  dataset.PL,dataset.PR))\n",
    "    \n",
    "    # r_vec and t_vec obtained are in camera coordinate frames (currState)\n",
    "    # we need to convert these matrix to world coordinates system (prevState)\n",
    "    opt_rvec_cam = (optRes.x[:3]).reshape(-1,1)\n",
    "    opt_tvec_cam = (optRes.x[3:]).reshape(-1,1)\n",
    "    opt_rmat_cam,_ = cv2.Rodrigues(opt_rvec_cam)\n",
    "    \n",
    "    # Obtain the pose of the camera (wrt state of the previous camera)\n",
    "    r_mat = opt_rmat_cam.T\n",
    "    t_vec = -opt_rmat_cam.T @ opt_tvec_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upating the pose of the camera of currState \n",
    "# According to the Vo tutorial by Dacvid Scaramuzza \n",
    "# C_n = C_n-1 * dTn-1\n",
    "# where the dTn-1 is in the coordinate frame of the second camera\n",
    "# Link : http://rpg.ifi.uzh.ch/docs/VO_Part_I_Scaramuzza.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "opencv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
