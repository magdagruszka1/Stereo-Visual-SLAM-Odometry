{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import scipy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "from stereoVO.structures import StateBolts, VO_StateMachine\n",
    "from stereoVO.datasets import KittiDataset\n",
    "from stereoVO.configs import yaml_parser\n",
    "from stereoVO.utils import rmse_error\n",
    "from stereoVO.optimization import get_minimization\n",
    "from stereoVO.geometry import (DetectionEngine, \n",
    "                               filter_matching_inliers, \n",
    "                               triangulate_points, \n",
    "                               filter_triangulated_points,\n",
    "                               project_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../../KITTI/KITTI_gray/dataset/sequences/00/'\n",
    "CONFIG_PATH =  '../configs/params.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KittiDataset(PATH)\n",
    "\n",
    "dataset.intrinsic = dataset.camera_intrinsic()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Configs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = yaml_parser(CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise the State**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_num = 0\n",
    "\n",
    "left_frame, right_frame, ground_truth = dataset[state_num]\n",
    "\n",
    "#initialise the state\n",
    "prevState = VO_StateMachine(state_num)\n",
    "\n",
    "#set frame state\n",
    "prevState.frames = left_frame, right_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detction Engine, Matching and Triangulation for first frame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_engine = DetectionEngine(prevState.frames.left, prevState.frames.right, params)\n",
    "\n",
    "prevState.matchedPoints, prevState.keypoints, prevState.descriptors = detection_engine.get_matching_keypoints()\n",
    "\n",
    "prevState.inliers, mask_epipolar = filter_matching_inliers(prevState.matchedPoints.left,\n",
    "                                            prevState.matchedPoints.right,\n",
    "                                            dataset.intrinsic,\n",
    "                                            params)\n",
    "\n",
    "prevState.pts3D, reproj_error = triangulate_points(prevState.inliers.left,\n",
    "                                    prevState.inliers.right,\n",
    "                                    dataset.PL,\n",
    "                                    dataset.PR)\n",
    "\n",
    "args_triangulation = params.geometry.triangulation\n",
    "prevState.pts3D_Filter, maskTriangulationFilter, ratioFilter = filter_triangulated_points(prevState.pts3D, reproj_error, **args_triangulation)\n",
    "\n",
    "prevState.InliersFilter = prevState.inliers.left[maskTriangulationFilter], prevState.inliers.right[maskTriangulationFilter]\n",
    "prevState.ratioTriangulationFilter = ratioFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the pose of the camera**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevState.location = np.array(params.initial.location)\n",
    "prevState.orientation = np.array(params.initial.orientation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_location = dataset.ground_truth[0][:,-1]\n",
    "ground_truth_orientation = dataset.ground_truth[0][:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.220922637035219e-16"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_error(prevState.location, ground_truth_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialise the Second State and set it as currState**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_num = 1\n",
    "\n",
    "left_frame, right_frame, ground_truth = dataset[state_num]\n",
    "\n",
    "#initialise the state\n",
    "currState = VO_StateMachine(state_num)\n",
    "\n",
    "#set frame state\n",
    "currState.frames = left_frame, right_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_engine = DetectionEngine(currState.frames.left, currState.frames.right, params)\n",
    "\n",
    "currState.matchedPoints, currState.keypoints, currState.descriptors = detection_engine.get_matching_keypoints()\n",
    "\n",
    "currState.inliers, mask_epipolar = filter_matching_inliers(currState.matchedPoints.left,\n",
    "                                            currState.matchedPoints.right,\n",
    "                                            dataset.intrinsic,\n",
    "                                            params)\n",
    "\n",
    "currState.pts3D, reproj_error = triangulate_points(currState.inliers.left,\n",
    "                                    currState.inliers.right,\n",
    "                                    dataset.PL,\n",
    "                                    dataset.PR)\n",
    "\n",
    "args_triangulation = params.geometry.triangulation\n",
    "currState.pts3D_Filter, maskTriangulationFilter, ratioFilter = filter_triangulated_points(currState.pts3D, reproj_error, **args_triangulation)\n",
    "\n",
    "currState.InliersFilter = currState.inliers.left[maskTriangulationFilter], currState.inliers.right[maskTriangulationFilter]\n",
    "currState.ratioTriangulationFilter = ratioFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracker = TrackingEngine(prev_frames, curr_frames, prevState.InliersFilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevFrames = prevState.frames\n",
    "currFrames = currState.frames\n",
    "prevInliers = prevState.InliersFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Tracking from prev state to current state**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lk_params = dict(winSize  = (21, 21), \n",
    "                 maxLevel = 5,\n",
    "                 criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 30, 0.03))\n",
    "\n",
    "dist_x_y = lambda x,y:np.sqrt(np.sum((x - y)**2, axis=1))\n",
    "\n",
    "def feature_tracking(imageRef, imageCur, pointsRef):\n",
    "    \n",
    "    \"\"\"\n",
    "    To Do : Add docstring for the function here.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert len(pointsRef.shape)==2 and pointsRef.shape[1]==2\n",
    "    \n",
    "    pointsRef = pointsRef.reshape(-1,1,2).astype('float32')\n",
    "    points_t0_t1, mask_t0_t1, err = cv2.calcOpticalFlowPyrLK(imageRef, imageCur, pointsRef, None, **lk_params)\n",
    "    \n",
    "    pointsRef = pointsRef.reshape(-1,2)\n",
    "    points_t0_t1 = points_t0_t1.reshape(-1,2)\n",
    "    mask_t0_t1 = mask_t0_t1.flatten().astype(bool)\n",
    "       \n",
    "#     pointsRef = pointsRef[mask_t0_t1]\n",
    "#     points_t0_t1 = points_t0_t1[mask_t0_t1]\n",
    "    \n",
    "    return pointsRef, points_t0_t1, mask_t0_t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsFilterLeft, pointsTrackedLeft, maskTrackingLeft = feature_tracking(prevFrames.left,\n",
    "                                                                        currFrames.left,\n",
    "                                                                        prevInliers.left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pointsFilterRight, pointsTrackedRight, maskTrackingRight = feature_tracking(prevFrames.right,\n",
    "                                                                           currFrames.right,\n",
    "                                                                           prevInliers.right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joint index and select only good tracked points\n",
    "maskTracking = np.logical_and(maskTrackingLeft, maskTrackingRight)\n",
    "pointsTracked = StateBolts(pointsTrackedLeft[maskTracking], pointsTrackedRight[maskTracking])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove non-valid points from inliers filtered in prev state\n",
    "prevStateInliers = StateBolts(prevInliers.left[maskTracking], prevInliers.right[maskTracking])\n",
    "pts3D_TrackingFilter = prevState.pts3D_Filter[maskTracking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, __ ) , mask_tracking_epipolar_left = filter_matching_inliers(prevStateInliers.left, pointsTracked.left, dataset.intrinsic, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(_, __ ) , mask_tracking_epipolar_right = filter_matching_inliers(prevStateInliers.right, pointsTracked.right, dataset.intrinsic, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_tracking_epipolar = np.logical_and(mask_tracking_epipolar_left, mask_tracking_epipolar_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currState.pointsTracked = (pointsTracked.left[mask_tracking_epipolar], pointsTracked.right[mask_tracking_epipolar])\n",
    "prevState.inliersTrackingFilter =  (prevStateInliers.left[mask_tracking_epipolar], prevStateInliers.right[mask_tracking_epipolar])\n",
    "prevState.pts3D_TrackingFilter = pts3D_TrackingFilter[mask_tracking_epipolar]                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**P3P Solver**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scale of translation of camera     : 0.6764176064655109\n",
      "Solution obtained in P3P Iteration : 1\n",
      "Ratio of Inliers                   : 1.0\n"
     ]
    }
   ],
   "source": [
    "args_pnpSolver = params.geometry.pnpSolver\n",
    "\n",
    "\n",
    "for i in range(args_pnpSolver.numTrials):\n",
    "    \n",
    "    retval, r_vec, t_vec, idxPose = cv2.solvePnPRansac(prevState.pts3D_TrackingFilter,\n",
    "                                                       currState.pointsTracked.left,\n",
    "                                                       dataset.intrinsic,\n",
    "                                                       None,\n",
    "                                                       iterationsCount=args_pnpSolver.numTrials,\n",
    "                                                       reprojectionError=args_pnpSolver.reprojectionError,\n",
    "                                                       confidence=args_pnpSolver.confidence,\n",
    "                                                       flags=cv2.SOLVEPNP_P3P)\n",
    "    \n",
    "    \n",
    "    r_mat, _ = cv2.Rodrigues(r_vec)\n",
    "    \n",
    "    # r_vec and t_vec obtained are in camera coordinate frames\n",
    "    # we need to convert these matrix in world coordinates system\n",
    "    # or we need to transforms the matrix from currState to prevState\n",
    "    \n",
    "    t_vec = -r_mat.T @ t_vec\n",
    "    r_mat = r_mat.T\n",
    "    \n",
    "    idxPose = idxPose.flatten().astype(bool)\n",
    "    \n",
    "    ratio = sum(idxPose)/len(idxPose)\n",
    "    scale = np.linalg.norm(t_vec)\n",
    "    \n",
    "    if scale<args_pnpSolver.deltaT and ratio>args_pnpSolver.minRatio:\n",
    "        print(\"Scale of translation of camera     : {}\".format(scale))\n",
    "        print(\"Solution obtained in P3P Iteration : {}\".format(i+1))\n",
    "        print(\"Ratio of Inliers                   : {}\".format(ratio))\n",
    "        break\n",
    "    else:\n",
    "        print(\"Warning : Max Iter : {} reached, still large position delta produced\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Non-linear Least Square Optimization of Pose**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimisation=True\n",
    "\n",
    "if optimisation:\n",
    "    \n",
    "    # Convert the matrix from world coordinates(prevState) to camera coordinates (currState)\n",
    "    t_vec = -r_mat.T @ t_vec\n",
    "    r_mat = r_mat.T\n",
    "    r_vec, _ = cv2.Rodrigues(r_mat)\n",
    "    \n",
    "    # Prepare an initial set of parameters to the optimizer\n",
    "    doF = np.concatenate((r_vec, t_vec)).flatten()\n",
    " \n",
    "    # Prepare the solver for minimization\n",
    "    optRes = least_squares(get_minimization, doF, method='lm', max_nfev=2000,\n",
    "                            args=(prevState.pts3D_TrackingFilter, \n",
    "                                  currState.pointsTracked.left, \n",
    "                                  currState.pointsTracked.right, \n",
    "                                  dataset.PL,dataset.PR))\n",
    "    \n",
    "    # r_vec and t_vec obtained are in camera coordinate frames (currState)\n",
    "    # we need to convert these matrix to world coordinates system (prevState)\n",
    "    opt_rvec_cam = (optRes.x[:3]).reshape(-1,1)\n",
    "    opt_tvec_cam = (optRes.x[3:]).reshape(-1,1)\n",
    "    opt_rmat_cam,_ = cv2.Rodrigues(opt_rvec_cam)\n",
    "    \n",
    "    # Obtain the pose of the camera (wrt state of the previous camera)\n",
    "    r_mat = opt_rmat_cam.T\n",
    "    t_vec = -opt_rmat_cam.T @ opt_tvec_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upating the pose of the camera of currState \n",
    "# According to the Vo tutorial by Dacvid Scaramuzza \n",
    "# C_n = C_n-1 * dTn-1\n",
    "# where the dTn-1 is in the coordinate frame of the second camera\n",
    "# Link : http://rpg.ifi.uzh.ch/docs/VO_Part_I_Scaramuzza.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pose Updation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "currState.orientation = prevState.orientation @ r_mat\n",
    "currState.location = prevState.orientation @ t_vec + prevState.location.reshape(-1,1)\n",
    "currState.location = currState.location.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_location = dataset.ground_truth[state_num][:,-1]\n",
    "ground_truth_orientation = dataset.ground_truth[state_num][:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseError = rmse_error(ground_truth_location, currState.location)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "opencv"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
